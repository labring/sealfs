# sealfs设计文档

## 系统架构
sealfs的架构为无中心架构，且无独立的元数据节点
sealfs包含三个组件，
- server负责文件以及元数据存储。
- client实现用户态的文件系统，对文件请求进行拦截并通过哈希算法进行存储寻址。
- manager负责协调集群。
设计图如下：
![](images/architecture.jpg)

### 全链路用户态
我们希望结合特定硬件从客户端文件请求劫持到网络到存储打造一个全链路用户态的分布式文件存储系统，从而获得极致的性能体验。

## 客户端
在客户端处，我们支持了两种类型的文件系统，一种是比较常见的fuse，另一种是用户态的文件系统，我们希望以这种方式提升性能。

### fuse
#### 内核文件系统
为了减少调用次数，一种实现方案则是直接使用内核态实现文件系统

1. 用户态请求
2. VFS
3. 内核态文件系统
4. 网络传输

这种方案可以将内核态和用户态切换次数减少，但缺点也显而易见：
1. 内核编程的调试复杂
2. 需要为客户端安装额外内核模块
3. 文件系统崩溃影响其他进程

#### fuse
为了避免上述问题，第一种方式我们采用容易实现且易于使用的fuse。

![alt fuse](https://imgconvert.csdnimg.cn/aHR0cHM6Ly9tbWJpei5xcGljLmNuL21tYml6X3BuZy9kNGhvWUpseE9qTnNvaWNRQkUwM01aRDBrWjNmY3VpYWVRZzJmV1RlNFlWV3RUYko5aWN1cG1iZ1IwZGd1RUlrTTloTzZzaWJQdU80VTlFNzlpYWczWWljdlE4US82NDA?x-oss-process=image/format,png)

在fuse中，通过网络进行文件存储的一次调用流程包括：
1. 用户态请求
2. VFS
3. fuse内核模块
4. fuse用户态模块
5. 网络传输

需要注意的是，在避免上述问题的同时，fuse也降低了文件系统的性能。


### 系统调用劫持
这是我们实现的第二种方案，即实现一个用户态的文件系统。在上图中，可以看到，用户请求并不是直接交给linux内核的，而是经过了glibc(或其他libc库)来提交系统调用，这意味着可以在libc层替换系统调用的地址，实现系统调用劫持。  
1. 用户态请求
2. 系统调用劫持
3. 网络传输

## 网络
对于网络部分，同样提供了两种网络传输方式，一种是RPC的方式，一种是通过RDMA的方式

### RPC
![image](https://user-images.githubusercontent.com/14962503/189853670-d10c29e8-34d7-468e-baa6-36c8fa65a3c9.png)

#### 寻址算法
一个文件请求会被客户端使用一致性哈希算法映射到一个服务器，通过长链接进行传输。

#### 请求流程

1. client接收请求，创建处理线程。创建处理线程的工作是由libfuse实现的，sealfs实现的函数可以认为已经是独立线程。
2. 计算文件所在的服务器。
3. 向server发送文件请求，hold线程。发送请求的过程要考虑多个请求并行处理的情况。为每个请求建立一个socket是最简单的实现，但创建连接的延迟过高，网络连接数也可能会过多。保持多个长连接保证了创建连接的延迟问题，但在大并发的情况下，依旧无法解决网络连接数量过多，同时代码的实现也稍显复杂。所以采用了一个长连接共享多个文件请求的方式。一个线程发送请求需要包含该请求的id与数据长度，同时需要实现一个额外的线程安全的队列用于保存发送请求后的线程的锁。
4. server处理文件请求，并将请求结果返回给client。处理过程中始终保持了请求的id。
5. client接收数据，激活请求线程并处理返回值。一个（或有限多个）独立的线程用于接收请求结果，其中包含了请求的id，需要在队列中查询请求id所对应的线程锁，写入结果并释放该线程锁，激活原请求线程并将结果返回给应用。

#### 内存拷贝

采用了多个请求共享同一线程的方式，socket发送请求时，由于数据不定长，需要提前发送一个长度变量，才能避免粘包。这里有两种不同的方案:  
一种是使用多个socket实现连接池，每次发送一个请求使用一个socket，该方案不存在数据包连续性的问题，可以多次发送。  
另一种是用同一个socket，但是要保证数据连续性，需要进行字符串拼接，涉及内存拷贝，开销会变大，那为了避免这个问题，每次发送数据需要给线程加锁，这个是第一个阶段的实现方案。

### RDMA

## 管理节点
管理节点用于管理server集群

### 心跳管理
server节点上下线的时候，会将心跳信息上报给管理节点，客户端会订阅心跳信息用于选址计算，同时，服务端也会订阅心跳信息用于数据迁移等方面

## 服务端
server主要存储两种类型的数据，一种是文件的元数据信息，以及文件本身的内容。

对于分布式文件存储而言，元数据无疑是热点数据，因此，我们采用分开挂盘的方式，将元数据和文件数据挂载在不同的盘中，一种经济的办法是将元数据数据挂载在SSD盘中，而对普通的文件数据存储在hdd中。当然，可以随意搭配。

### 元数据管理

>在大数据环境下，元数据的体量也非常大，元数据的存取性能是整个分布式文件系统性能的关键。常见的元数据管理可以分为集中式和分布式元数据管理架构。集中式元数据管理架构采用单一的元数据服务器，实现简单．但是存在单点故障等问题。分布式元数据管理架构则将元数据分散在多个结点上．进而解决了元数据服务器的性能瓶颈等问题．并提高了元数据管理架构的可扩展性，但实现较为复杂，并引入了元数据一致性的问题。另外，还有一种无元数据服务器的分布式架构，通过在线算法组织数据，不需要专用的元数据服务器。但是该架构对数据一致性的保障很困难．实现较为复杂。文件目录遍历操作效率低下，并且缺乏文件系统全局监控管理功能。

sealfs目前选择的是元数据节点的架构，即避免了元数据节点单点故障的问题，但是对于元数据的遍历成为了一个难题。

#### 元数据持久化内存存储

为了提高元数据的性能，我们打算结合支持持久化内存的硬件对元数据进行存储设计。

### 数据存储

#### 绕过本地文件系统
为了提升性能，sealfs直接跨过文件系统进行文件存储。当然这会带来更多的复杂性。

#### 适配不同的硬件
对于不同的固态硬盘有不同的特性，我们会对不同的硬件都进行适配，设计不同的数据结构，希望对用户使用的每一种硬件都达到比较好的效果

## 一些其他的扩展
这些拓展点暂时不在第一版计划实现中
- 数据可靠性与高可用  
  - 多副本
    多副本暂时计划采用raft协议，由一致性hash算法计算副本位置分布到多个节点上实现replica。
  - 纠删码

- 数据扩缩容  
基于一致性hash实现扩缩容，具体细节暂时先不讲。需要明确的是添加或删除节点后集群会进行rebalance，这是一致性hash本身需要做的，无需额外设计。rebalance期间会导致集群性能下降，且可能耗费较长时间，但对于可以持续提供服务。在rebalance期间需要做的工作如下：

| 开始扩容 | 迁移数据 | 扩容完成 |
| ---- | ---- | ---- |
| 更新集群元数据 | client进行二次请求，确认迁移后数据和迁移前数据一致性，并将数据写于新节点;同时迁移任务进行数据迁移和同步 | 确认集群元数据 |

- 租户管理  
对于不同的client申请挂载的磁盘，进行容量限制隔离




